# Exploring Multimodal Emotion Recognition on Social Media Content

The code of this repository is used for my thesis for Master Degree in Computer Science AI curriculum at University of Pisa.<br>

A multimodal model based on CLIP-like models is defined, to perform Multimodal Emotion Recognition on text-image pairs extracted from tweets. Many variations of this model are explored, along with single modality models (BERT and ViT) and a simple zero-shot approach using LLaVA-NeXT.<br>

The dataset was created by Roberto Cannarella for his Master Degree thesis.<br>
