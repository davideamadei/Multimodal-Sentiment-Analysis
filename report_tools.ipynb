{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "\n",
    "def get_support(dataset, labels, bert=False):\n",
    "    support = {label:0 for label in labels}\n",
    "    support[\"total\"] = 0\n",
    "    for i, row in dataset.iterrows():\n",
    "        for label in labels:\n",
    "            if row[label]:\n",
    "                support[label] += 1\n",
    "                support[\"total\"] += 1\n",
    "    return support\n",
    "\n",
    "dataset, _ = MulTweEmoDataset.load(csv_path=\"dataset/MulTweEmo.csv\", test_split = None)\n",
    "train, _ = MulTweEmoDataset.load(csv_path=\"dataset/train_MulTweEmo.csv\", test_split = None)\n",
    "test, _ = MulTweEmoDataset.load(csv_path=\"dataset/test_MulTweEmo.csv\", test_split = None)\n",
    "val, _ = MulTweEmoDataset.load(csv_path=\"dataset/val_MulTweEmo.csv\", test_split = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(test[\"labels\"].to_list()).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_only = True\n",
    "if text_only:\n",
    "    dataset = dataset.drop_duplicates(subset=[\"id\"])\n",
    "    train = train.drop_duplicates(subset=[\"id\"])\n",
    "    test = test.drop_duplicates(subset=[\"id\"])\n",
    "    val = val.drop_duplicates(subset=[\"id\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_support = get_support(dataset, labels)\n",
    "train_support = get_support(train, labels)\n",
    "test_support = get_support(test, labels)\n",
    "val_support = get_support(val, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_dict = {\"Train\": train_support, \"Test\": test_support, \"Val\": val_support, \"Dataset\": dataset_support}\n",
    "support_table = pd.DataFrame(support_dict)\n",
    "support_table.index = support_table.index.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percent_support_dict = {key: {label: value[label]/value[\"total\"] for label in value.keys()} for key, value in support_dict.items()}\n",
    "percent_support_table = pd.DataFrame(percent_support_dict)\n",
    "percent_support_table.index = percent_support_table.index.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(support_table.to_latex())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gold and silver label comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_silver_dataset(raw_dataset_path=\"./dataset/MulTweEmo_raw.pkl\",\n",
    "                        csv_path=\"./dataset/silver_MulTweEmo.csv\",  \n",
    "                        mode=\"label\",\n",
    "                        label_name=\"multi_label\",\n",
    "                        seed_threshold=0.81,\n",
    "                        top_seeds:(int|dict)=None):\n",
    "        \n",
    "    if mode != \"threshold\" and mode != \"label\":\n",
    "        raise ValueError(\"mode must be chosen between \\\"top\\\", \\\"threhsold\\\" or \\\"label\\\"\")\n",
    "\n",
    "    with open(raw_dataset_path, 'rb') as file:\n",
    "        dataset = pd.compat.pickle_compat.load(file)\n",
    "\n",
    "    dataset = dataset[dataset[\"M_Anger\"].notnull()].copy().reset_index(drop=True)\n",
    "    \n",
    "    dataset = dataset.drop(columns = [\"M_gold_multi_label\", \"T_gold_multi_label\"])\n",
    "    dataset[\"img_count\"] = dataset[\"path_photos\"].apply(len)\n",
    "\n",
    "    labels = MulTweEmoDataset.get_labels(drop_something_else=True)\n",
    "    labels.remove(\"neutral\")\n",
    "    \n",
    "    emotions_m = {emotion: \"M_\"+emotion.capitalize() for emotion in labels}\n",
    "    emotions_t = {emotion: \"T_\"+emotion.capitalize() for emotion in labels}\n",
    "    \n",
    "    label_columns = list(emotions_m.values()) + list(emotions_t.values())\n",
    "    columns = [\"id\", \"tweet\", \"img_count\", \"seeds\"] + label_columns\n",
    "\n",
    "    dataset[list(emotions_t.values())] = 0\n",
    "\n",
    "    if mode==\"label\":\n",
    "        columns\n",
    "        def set_labels(row):\n",
    "            if label_name == \"multi_label\":\n",
    "                for label in row[label_name]:\n",
    "                    row[emotions_t[label]] = 1\n",
    "            elif label_name == \"uni_label\":\n",
    "                label = row[label_name]\n",
    "                row[emotions_t[label]] = 1\n",
    "            else:\n",
    "                raise ValueError()\n",
    "            return row\n",
    "    else:\n",
    "        def set_labels(row):\n",
    "            for e, d in row[\"seeds\"].items():\n",
    "                avg = sum(d.values())/len(d.values())\n",
    "                if avg > seed_threshold:\n",
    "                    row[emotions_t[e]] = 1\n",
    "            return row\n",
    "        \n",
    "    def seeds_avg(row):\n",
    "        avgs = {}\n",
    "        for e, d in row[\"seeds\"].items():\n",
    "            avgs[e] = sum(d.values())/len(d.values())\n",
    "        row[\"avg_seeds\"] = avgs\n",
    "        return row\n",
    "    \n",
    "    dataset = dataset.apply(set_labels, axis=1)\n",
    "    \n",
    "    dataset = dataset.apply(seeds_avg, axis=1)\n",
    "\n",
    "    if top_seeds != None:\n",
    "        labels.remove(\"neutral\")\n",
    "        labels.remove(\"something else\")\n",
    "        indices = []\n",
    "        if type(top_seeds) == int:\n",
    "            for label in labels:\n",
    "                indices += pd.DataFrame(dataset[\"avg_seeds\"].to_list()).sort_values(by=label, ascending=False).head(top_seeds).index.to_list()\n",
    "        else:\n",
    "            for label, top_n in top_seeds.items():\n",
    "                indices += pd.DataFrame(dataset[\"avg_seeds\"].to_list()).sort_values(by=label, ascending=False).head(top_n).index.to_list()\n",
    "        dataset = dataset.iloc[indices].sort_index().drop_duplicates(subset=\"id\")\n",
    "    dataset = dataset[columns+[\"avg_seeds\"]]\n",
    "\n",
    "    \n",
    "    for label in emotions_m.values():\n",
    "        dataset[label] = dataset[label].apply(lambda x: 1 if x>=2 else 0)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "labels = MulTweEmoDataset.get_labels()\n",
    "labels.remove(\"neutral\")\n",
    "\n",
    "gs_dataset = create_silver_dataset(\n",
    "    label_name=\"uni_label\",\n",
    "    # mode=\"threshold\",\n",
    "    # seed_threshold=0.84\n",
    "    )\n",
    "\n",
    "gs_dataset[\"label_silver\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"T_\")])\n",
    "gs_dataset[\"label_gold\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"M_\")])\n",
    "\n",
    "emotions_t = {emotion: \"T_\"+emotion.capitalize() for emotion in labels}\n",
    "gs_dataset = gs_dataset[(gs_dataset[emotions_t.values()].sum(axis=1))!=0]\n",
    "\n",
    "class_report = skm.classification_report(np.array(gs_dataset[\"label_gold\"].to_list()), np.array(gs_dataset[\"label_silver\"].to_list()), target_names=labels, zero_division=0, output_dict=True)\n",
    "class_report = pd.DataFrame(class_report).T\n",
    "class_report[\"support\"] = class_report[\"support\"].astype(int)\n",
    "print(class_report.to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "labels = MulTweEmoDataset.get_labels()\n",
    "labels.remove(\"neutral\")\n",
    "\n",
    "gs_dataset = create_silver_dataset(\n",
    "    label_name=\"multi_label\",\n",
    "    )\n",
    "\n",
    "def get_precision(threshold, gs_dataset):\n",
    "    \n",
    "    emotions_t = {emotion: \"T_\"+emotion.capitalize() for emotion in labels}\n",
    "\n",
    "    def set_labels(row):\n",
    "        for e, v in row[\"avg_seeds\"].items():\n",
    "            if v > threshold:\n",
    "                row[emotions_t[e]] = 1\n",
    "        return row\n",
    "    \n",
    "    gs_dataset[list(emotions_t.values())] = 0\n",
    "    gs_dataset = gs_dataset.apply(set_labels, axis=1)\n",
    "\n",
    "    gs_dataset[\"label_silver\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"T_\")])\n",
    "    gs_dataset[\"label_gold\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"M_\")])\n",
    "\n",
    "    tmp_dataset = gs_dataset[(gs_dataset[\"label_silver\"].apply(sum))!=0]\n",
    "\n",
    "    class_report = skm.classification_report(np.array(tmp_dataset[\"label_gold\"].to_list()), np.array(tmp_dataset[\"label_silver\"].to_list()), target_names=labels, zero_division=0, output_dict=True)\n",
    "    class_report = pd.DataFrame(class_report).T\n",
    "    class_report[\"support\"] = class_report[\"support\"].astype(int)\n",
    "    return class_report[\"precision\"][\"samples avg\"]\n",
    "\n",
    "def get_support(threshold, gs_dataset):\n",
    "    \n",
    "    emotions_t = {emotion: \"T_\"+emotion.capitalize() for emotion in labels}\n",
    "\n",
    "    def set_labels(row):\n",
    "        for e, v in row[\"avg_seeds\"].items():\n",
    "            if v > threshold:\n",
    "                row[emotions_t[e]] = 1\n",
    "        return row\n",
    "    \n",
    "    gs_dataset[list(emotions_t.values())] = 0\n",
    "    gs_dataset = gs_dataset.apply(set_labels, axis=1)\n",
    "\n",
    "    gs_dataset[\"label_silver\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"T_\")])\n",
    "    gs_dataset[\"label_gold\"] = MulTweEmoDataset._build_label_matrix(gs_dataset, gs_dataset.columns[gs_dataset.columns.str.startswith(\"M_\")])\n",
    "\n",
    "    tmp_dataset = gs_dataset[(gs_dataset[\"label_silver\"].apply(sum))!=0]\n",
    "\n",
    "    return tmp_dataset.shape[0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of labels by emotion for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "labels.remove(\"neutral\")\n",
    "\n",
    "count_dict = {e: [0]*len(labels) for e in labels}\n",
    "for e1 in labels:\n",
    "    tmp_labels = labels.copy()\n",
    "    tmp_labels.remove(e1)\n",
    "    for i, row in gs_dataset.iterrows():\n",
    "        e_count = 0\n",
    "        if row[emotions_t[e1]]:\n",
    "            for e2 in tmp_labels:\n",
    "                if row[emotions_t[e2]]:\n",
    "                    e_count += 1\n",
    "            count_dict[e1][e_count] += 1\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = list(range(1,len(labels)+1))\n",
    "avg_count_dict = {e: sum([a*b for a,b in zip(n_labels, v)])/sum(v) for e,v in count_dict.items()}\n",
    "avg_count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0.4, 0.95, 100)\n",
    "\n",
    "y = [get_precision(v, gs_dataset) for v in x]\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Samples average precision\")\n",
    "plt.grid()\n",
    "\n",
    "ax.axhline(0, color='#777777')\n",
    "ax.axvline(0, color='#777777')\n",
    "ax.set_xticks([x/10 for x in range(4, 11)])\n",
    "ax.set_xlim((0.5, 1))\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "x = np.linspace(0.4, 0.95, 100)\n",
    "\n",
    "y = [get_support(v, gs_dataset) for v in x]\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlabel(\"Threshold\")\n",
    "ax.set_ylabel(\"Number of tweets\")\n",
    "plt.grid()\n",
    "\n",
    "ax.axhline(0, color='#777777')\n",
    "ax.axvline(0, color='#777777')\n",
    "ax.set_xticks([x/10 for x in range(4, 11)])\n",
    "ax.set_xlim((0.5, 1))\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of labels by emotion for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "dataset, _ = MulTweEmoDataset.load(csv_path=\"dataset/MulTweEmo.csv\", test_split = None)\n",
    "\n",
    "count_dict = {e: [0]*len(labels) for e in labels}\n",
    "for e1 in labels:\n",
    "    tmp_labels = MulTweEmoDataset.get_labels()\n",
    "    tmp_labels.remove(e1)\n",
    "    for i, row in dataset.iterrows():\n",
    "        e_count = 0\n",
    "        if row[e1] == 1:\n",
    "            for e2 in tmp_labels:\n",
    "                if row[e2]:\n",
    "                    e_count += 1\n",
    "            count_dict[e1][e_count] += 1\n",
    "count_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dataset.drop_duplicates(subset=\"id\")[labels].sum(axis=1)).sum()/804"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = list(range(1,len(labels)+1))\n",
    "avg_count_dict = {e: sum([a*b for a,b in zip(n_labels, v)])/sum(v) for e,v in count_dict.items()}\n",
    "avg_count_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Caption examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "import pandas as pd\n",
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "dataset, _ = MulTweEmoDataset.load(csv_path=\"dataset/MulTweEmo.csv\", test_split = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampled_dataset = pd.DataFrame()\n",
    "for e in labels:\n",
    "    sampled_dataset = pd.concat([sampled_dataset, dataset[dataset[e]==1].sample(random_state=1)])\n",
    "sampled_dataset = pd.concat([sampled_dataset, dataset[dataset[e]==1].sample(n=3, random_state=1)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_labels(row):\n",
    "    id2label = MulTweEmoDataset.get_id2label()\n",
    "    labels = []\n",
    "    for i, label in enumerate(row[\"labels\"]):\n",
    "        if label:\n",
    "            labels.append(id2label[i].capitalize())\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = \"\"\n",
    "image_list = []\n",
    "build_path = lambda x: \"images/dataset/captions/\" + re.search(\"(?<=./dataset/images/)(.*)\", x).group(1)\n",
    "includegraphics = lambda x: f\"\\\\rowincludegraphics[width=0.2\\\\textwidth]{{{x}}}\"\n",
    "count = 0\n",
    "for i in range(len(sampled_dataset)//4):\n",
    "    table += \"\\\\textbf{Image}\"\n",
    "    for j, row in sampled_dataset[i*4: (i+1)*4].iterrows():\n",
    "        table += \" & \" + includegraphics(build_path(row[\"img_path\"]))\n",
    "        image_list.append(row[\"img_path\"])\n",
    "    table += \"\\\\\\\\\\n\\\\addlinespace \\\\hline \\\\addlinespace\\n\"\n",
    "    \n",
    "    table += \"\\\\textbf{Caption}\"\n",
    "    for j, row in sampled_dataset[i*4: (i+1)*4].iterrows():\n",
    "        table += \" & \" + row[\"caption\"]\n",
    "    table += \"\\\\\\\\\\n\\\\addlinespace \\\\hline \\\\addlinespace\\n\"\n",
    "    \n",
    "    table += \"\\\\textbf{Labels}\"\n",
    "    for j, row in sampled_dataset[i*4: (i+1)*4].iterrows():\n",
    "        table += \" & \" + \", \".join(get_labels(row))\n",
    "    table += \"\\\\\\\\\\n\\\\addlinespace \\\\hline \\\\addlinespace\\n\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "src_files = image_list\n",
    "for file_name in src_files:\n",
    "    # full_file_name = os.path.join(src, file_name)\n",
    "    if os.path.isfile(file_name):\n",
    "        shutil.copy(file_name, \"C:/Users/Utente/Desktop/Multimodal-Sentiment-Analysis/Tesi/images/dataset/captions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 trials for each metric baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = \"bert\"\n",
    "df = pd.read_csv(f\"Report1/{file}_study.csv\")\n",
    "df = df.drop(columns=[\"State\", \"Number\"])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_prefix(text, prefix):\n",
    "    if text.startswith(prefix):\n",
    "        return text[len(prefix):]\n",
    "    return text\n",
    "columns = df.columns.to_list()\n",
    "columns = [remove_prefix(column, \"Param \") for column in columns]\n",
    "columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.sort_values(by=\"loss\", ascending=True if \"loss\"==\"loss\" else False).head(10).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metric = \"loss\"\n",
    "# metric = \"f1_score\"\n",
    "metric = \"exact_match\"\n",
    "\n",
    "print((df.sort_values(by=metric, ascending=True if metric==\"loss\" else False).head(10).to_latex(header=columns,\n",
    "                                                                        caption=f\"Top 10 {metric.replace('_', ' ')} results for {file.replace('_', ' ')}\",\n",
    "                                                                        # label=f\"tab:{file}_top_10\",\n",
    "                                                                        longtable=False)).replace(\"_\", \"\\_\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of best trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement widget drop down menu from studies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "import optuna\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_name = \"sqlite:///final_study_2.db\"\n",
    "storage_name = \"sqlite:///MulTweEmo_study_new_split.db\"\n",
    "\n",
    "options = optuna.study.get_all_study_names(storage_name)\n",
    "dropdown = widgets.Dropdown(options=options) \n",
    "study_name=dropdown.value\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "trials = study.best_trials\n",
    "\n",
    "def on_change(change):\n",
    "    global study_name\n",
    "    global study\n",
    "    global trials\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        study_name = change['new']\n",
    "        clear_output()\n",
    "        display(dropdown)\n",
    "        study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "        trials = study.best_trials\n",
    "\n",
    "dropdown.observe(on_change)\n",
    "display(dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials_dict = {model:studies[model].best_trials for model in models}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trials = trials_dict[\"base\"]\n",
    "# trials.sort(key = lambda x, : x.user_attrs[\"samples avg\"][\"f1-score\"], reverse=True)\n",
    "# trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = list(study.best_trials[0].params.keys())\n",
    "keys = [\"number\"] + study.metric_names + params\n",
    "# keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trials2dict(trials, keys):\n",
    "    best_trials_dict = {key:[] for key in keys}\n",
    "    \n",
    "    for trial in trials:\n",
    "        best_trials_dict[\"number\"].append(trial.number)\n",
    "        for i, metric in enumerate(study.metric_names):\n",
    "            best_trials_dict[metric].append(trial.values[i])\n",
    "        for param in params:\n",
    "            # if model == \"siglip\" and param == \"batch_size\":\n",
    "            #     best_trials_dict[param].append(8)\n",
    "            # else:\n",
    "            best_trials_dict[param].append(trial.params[param])\n",
    "\n",
    "    return best_trials_dict\n",
    "best_trials = trials2dict(study.best_trials, keys)\n",
    "# best_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_eye(x):\n",
    "    return x\n",
    "def format_float(x):\n",
    "    return '%.4f' % x\n",
    "def format_long_float(x):\n",
    "    return '%.2e' % x\n",
    "\n",
    "formatters={key: format_eye for key in keys}\n",
    "formatters[\"learning_rate\"] = format_long_float\n",
    "formatters[\"Loss\"] = format_float\n",
    "formatters[\"F1-score\"] = format_float\n",
    "formatters[\"exact_match\"] = format_float\n",
    "formatters[\"Accuracy\"] = formatters.pop(\"exact_match\")\n",
    "formatters[\"dropout\"] = format_float\n",
    "# formatters = list(formatters.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame.from_dict(best_trials).rename(columns={\"number\": \"Trial\", \"loss\": \"Loss\", \"f1_score\": \"F1-score\", \"exact_match\": \"Accuracy\"})\n",
    "print(\"\\\\begin{adjustbox}{width=\\\\textwidth,center=\\\\textwidth}\")\n",
    "tmp_df = tmp_df.style.highlight_max(axis=0, props=\"textit:--rwrap;\", subset=[\"F1-score\", \"Accuracy\"])\n",
    "tmp_df = tmp_df.highlight_min(axis=0, props=\"textit:--rwrap;\", subset=[\"Loss\"])\n",
    "tmp_df = tmp_df.format(formatter=formatters)\n",
    "tmp_df = tmp_df.hide(axis=\"index\")\n",
    "print((tmp_df.to_latex(hrules=True, column_format=\"r|rrr|\"+\"r\"*len(params))).replace(\"_\", \"\\_\"))\n",
    "print(\"\\\\end{adjustbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_df = pd.DataFrame.from_dict(best_trials).rename(columns={\"exact_match\": \"accuracy\", \"number\": \"trial\"})\n",
    "tmp_df = tmp_df[[\"trial\", \"loss\", \"f1_score\", \"accuracy\"]]\n",
    "print(\"\\\\begin{adjustbox}{width=\\\\textwidth,center=\\\\textwidth}\")\n",
    "tmp_df = tmp_df.style.highlight_max(axis=0, props=\"textbf:--rwrap;\", subset=[\"f1_score\", \"accuracy\"])\n",
    "tmp_df = tmp_df.highlight_min(axis=0, props=\"textbf:--rwrap;\", subset=[\"loss\"])\n",
    "tmp_df = tmp_df.format(formatter=formatters)\n",
    "tmp_df = tmp_df.hide(axis=\"index\")\n",
    "print((tmp_df.to_latex(hrules=True)).replace(\"_\", \" \"))\n",
    "print(\"\\\\end{adjustbox}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame.from_dict(best_trials)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Individual emotion analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "import optuna\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_name = \"sqlite:///final_study.db\"\n",
    "storage_name = \"sqlite:///MulTweEmo_study_new_split.db\"\n",
    "\n",
    "metric = \"loss\"\n",
    "\n",
    "options = optuna.study.get_all_study_names(storage_name)\n",
    "dropdown = widgets.Dropdown(options=options) \n",
    "study_name=dropdown.value\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "trials = study.trials\n",
    "trials.sort(key = lambda x, : x.values[1], reverse=True)\n",
    "\n",
    "def on_change(change):\n",
    "    global study_name\n",
    "    global study\n",
    "    global trials\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        study_name = change['new']\n",
    "        clear_output()\n",
    "        display(dropdown)\n",
    "        study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "        trials = study.trials\n",
    "        trials.sort(key = lambda x, : x.values[1], reverse=True)\n",
    "\n",
    "dropdown.observe(on_change)\n",
    "display(dropdown)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotly.io import show\n",
    "\n",
    "fig = optuna.visualization.plot_param_importances(study, target_name=[\"Loss\", \"F1-score\", \"Accuracy\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1350,\n",
    "    height=450,\n",
    "    title=None,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font_size=17,\n",
    ")\n",
    "\n",
    "newnames = {\"loss\": \"Loss\", \"f1_score\": \"F1-score\", \"exact_match\" :\"Accuracy\"}\n",
    "fig.for_each_trace(lambda t: t.update(name = newnames[t.name]))\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from plotly.io import from_json\n",
    "\n",
    "targets = [\"Loss\", \"F1-score\", \"Accuracy\"]\n",
    "fig = optuna.visualization.plot_parallel_coordinate(study, target=lambda x: x.values[0], target_name=\"Loss\")\n",
    "fig_json = json.loads(fig.to_json())\n",
    "\n",
    "for i in range(1, len(targets)):\n",
    "    tmp_fig = optuna.visualization.plot_parallel_coordinate(study, target=lambda x: x.values[i], target_name=targets[i])\n",
    "    tmp_fig_json = json.loads(tmp_fig.to_json())\n",
    "    fig_json[\"data\"][0][\"dimensions\"].insert(i, tmp_fig_json[\"data\"][0][\"dimensions\"][0])\n",
    "\n",
    "fig = from_json(json.dumps(fig_json))\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1200,\n",
    "    height=450,\n",
    "    title=None,\n",
    "    # margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font_size=14,\n",
    ")\n",
    "\n",
    "import plotly.express as px\n",
    "# print(px.colors.sequential.Reds)\n",
    "fig.data[0].line.colorscale = px.colors.sequential.Reds\n",
    "# fig.data[0].line.reversescale = not fig.data[0].line.reversescale\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trials2dict(trials):\n",
    "    params = list(study.best_trials[0].params.keys())\n",
    "    keys = [\"number\"] + params \n",
    "    trial_dict = {key: [] for key in keys}\n",
    "\n",
    "    for trial in trials:\n",
    "        trial_dict[\"number\"].append(trial.number)\n",
    "        for param in params:\n",
    "            # if model == \"siglip\" and param == \"batch_size\":\n",
    "            #     best_trials_dict[param].append(8)\n",
    "            # else:\n",
    "            trial_dict[param].append(trial.params[param])\n",
    "\n",
    "    return trial_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# trials.sort(key = lambda x, : x.values[0], reverse=True)\n",
    "# top_trials = trials[:10]\n",
    "trials.sort(key = lambda x, : x.values[1], reverse=True)\n",
    "top_trials = trials[:10]\n",
    "trials.sort(key = lambda x, : x.values[2], reverse=True)\n",
    "top_trials += trials[:10]\n",
    "top_trials = pd.DataFrame(trials2dict(top_trials))\n",
    "top_trials = top_trials.drop_duplicates()\n",
    "summary = top_trials.describe()\n",
    "summary = summary.drop(columns=[\"number\"])\n",
    "row_list = summary.index.to_list()\n",
    "row_list.remove(\"count\")\n",
    "print(top_trials.shape)\n",
    "print(summary.loc[row_list].to_latex(float_format=\"%.4g\", escape=True))\n",
    "summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "labels = MulTweEmoDataset.get_labels()\n",
    "trials.sort(key = lambda x, : x.values[1], reverse=True)\n",
    "trial_attr = trials[0].user_attrs.copy()\n",
    "support = {label: int(trial_attr[label][\"support\"]) for label in labels}\n",
    "support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_metrics = trials[0].user_attrs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_pred_samples = emotion_metrics.pop(\"no_prediction_samples\")\n",
    "no_pred_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_metrics_table = pd.DataFrame(emotion_metrics).T\n",
    "emotion_metrics_table = emotion_metrics_table.loc[labels + [metric for metric in emotion_metrics_table.index if metric not in labels]]\n",
    "emotion_metrics_table[\"support\"] = emotion_metrics_table[\"support\"].astype(int)\n",
    "print(emotion_metrics_table.to_latex(float_format=\"%.5f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for emotion, metrics in emotion_metrics.items():\n",
    "#     metrics[\"support\"] /= sum(support.values())\n",
    "# emotion_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_metrics_table = emotion_metrics_table.drop(columns=[\"support\"])\n",
    "emotion_metrics_table.columns = emotion_metrics_table.columns.map(str.capitalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_agg_list = [\"macro avg\", \"micro avg\", \"weighted avg\", \"samples avg\"]\n",
    "ax = emotion_metrics_table.loc[labels + f1_agg_list].plot(kind=\"bar\", figsize=(7,4), yticks=[x / 10 for x in range(0,11)])\n",
    "# ax = emotion_metrics_table.loc[labels].plot(kind=\"bar\", figsize=(6,4), yticks=[x / 10 for x in range(0,11)])\n",
    "ax.set_xticklabels([l.get_text().capitalize() for l in ax.get_xticklabels()], rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "tmp = []\n",
    "no_pred_samples_tot = 0\n",
    "for i in range(10):\n",
    "    no_pred_samples_tot += trials[i].user_attrs[\"no_prediction_samples\"]\n",
    "no_pred_samples_var = 0\n",
    "for i in range(10):\n",
    "    no_pred_samples_var += (trials[i].user_attrs[\"no_prediction_samples\"]-no_pred_samples_tot/10)**2\n",
    "print(sqrt(no_pred_samples_var/10))\n",
    "print(no_pred_samples_tot/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = []\n",
    "# trials.sort(key = lambda x, : x.values[0], reverse=False)\n",
    "trials.sort(key = lambda x, : x.values[1], reverse=True)\n",
    "for i in range(10):\n",
    "    tmp.append(pd.DataFrame(trials[i].user_attrs).drop(columns=\"no_prediction_samples\").T.drop(columns=[\"support\"]))\n",
    "best_avg_metrics = pd.concat(tmp).groupby(level=0).mean()\n",
    "best_avg_metrics.columns = [c.capitalize() for c in best_avg_metrics.columns]\n",
    "best_avg_metrics = best_avg_metrics.loc[labels]\n",
    "ax = best_avg_metrics.plot(kind=\"bar\", figsize=(6,4), yticks=[x / 10 for x in range(0,11)])\n",
    "ax.set_xticklabels([l.get_text().capitalize() for l in ax.get_xticklabels()], rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "ax.legend(fontsize=12, loc=\"upper right\")\n",
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "ax.yaxis.grid(True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Top 10 metrics average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import widgets\n",
    "import optuna\n",
    "from IPython.display import clear_output\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_name = \"sqlite:///MulTweEmo_study_new_split.db\"\n",
    "\n",
    "metrics = [\"loss\", \"f1-score\", \"accuracy\"]\n",
    "\n",
    "options = optuna.study.get_all_study_names(storage_name)\n",
    "dropdown = widgets.Dropdown(options=options) \n",
    "study_name=dropdown.value\n",
    "\n",
    "study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "\n",
    "trials = {metric: study.trials for metric in metrics}\n",
    "for i, metric in enumerate(metrics):\n",
    "    trials[metric].sort(key = lambda x: x.values[i], reverse=True if metric != \"loss\" else False)\n",
    "\n",
    "def on_change(change):\n",
    "    global study_name\n",
    "    global study\n",
    "    global trials\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        study_name = change['new']\n",
    "        clear_output()\n",
    "        display(dropdown)\n",
    "        study = optuna.create_study(study_name=study_name, storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "        trials = {metric: study.trials for metric in metrics}\n",
    "        for i, metric in enumerate(metrics):\n",
    "            trials[metric].sort(key = lambda x: x.values[i], reverse=True if metric != \"loss\" else False)\n",
    "\n",
    "dropdown.observe(on_change)\n",
    "display(dropdown)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg = {metric: {metric_2: 0 for metric_2 in metrics} for metric in metrics}\n",
    "top_n = 10\n",
    "for metric_2 in metrics:\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for j in range(top_n):\n",
    "            avg[metric_2][metric] += trials[metric_2][j].values[i]\n",
    "        avg[metric_2][metric] /= top_n\n",
    "avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "std = {metric: {metric_2: 0 for metric_2 in metrics} for metric in metrics}\n",
    "\n",
    "for metric_2 in metrics:\n",
    "    for i, metric in enumerate(metrics):\n",
    "        for j in range(top_n):\n",
    "            std[metric_2][metric] += (trials[metric_2][j].values[i] - avg[metric_2][metric]) ** 2\n",
    "        std[metric_2][metric] /= top_n\n",
    "        std[metric_2][metric] = math.sqrt(std[metric_2][metric])\n",
    "std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model comparison by emotion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "import pandas as pd\n",
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# storage_name = \"sqlite:///final_study.db\"\n",
    "storage_name = \"sqlite:///MulTweEmo_study_new_split.db\"\n",
    "\n",
    "study_name_list = [\n",
    "    \"base\", \n",
    "    \"jina\", \n",
    "    \"large\", \n",
    "    \"siglip\", \n",
    "    \"blip2\"\n",
    "    ]\n",
    "\n",
    "# study_name_list = [\n",
    "#     \"base\",\n",
    "#     # \"base_text_only\",\n",
    "#     # \"jina\",\n",
    "#     \"base_freeze-weights\",\n",
    "#     \"base_append-captions\",\n",
    "#     # \"jina_append-captions\",\n",
    "#     # \"base_freeze-weights_augment_0.84\",\n",
    "#     \"base_process_emojis\",\n",
    "#     \"base_append-captions_process_emojis\",\n",
    "#     \"base_augment_0.82\",\n",
    "#     \"base_augment_0.84\",\n",
    "#     # \"base_append-captions_augment_0.84\",\n",
    "#     # \"base_append-captions_freeze-weights_augment_0.84\"\n",
    "#     ]\n",
    "\n",
    "# study_name_list = [\n",
    "#     \"bert_final\",\n",
    "#     \"base_final\",\n",
    "#     \"base_augment_final\",\n",
    "#     \"base_captions_final\"\n",
    "#     ]\n",
    "\n",
    "\n",
    "# study_name_list = [\"base_augment_0.82\", \"base_augment_0.84\"]\n",
    "\n",
    "studies = {}\n",
    "trials = {}\n",
    "\n",
    "metric = 1\n",
    "\n",
    "for name in study_name_list:\n",
    "    studies[name] = optuna.create_study(study_name=name+\"_study\", storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "    trials[name] = studies[name].trials\n",
    "    if name == \"base_append-captions_freeze-weights_augment_0.84\":\n",
    "        trials[name].pop(1)\n",
    "        trials[name].pop(0)\n",
    "    trials[name].sort(key = lambda x: x.values[metric], reverse=False if metric==0 else True)\n",
    "# trials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import optuna\n",
    "from plotly.io import show\n",
    "fig = optuna.visualization.plot_edf(list(studies.values()), target=lambda x: x.values[metric], target_name=\"Accuracy\")\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=750,\n",
    "    height=450,\n",
    "    title=None,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font_size=17,\n",
    ")\n",
    "\n",
    "# names_list = [\n",
    "#     \"Base CLIP\",\n",
    "#     \"Jina CLIP\",\n",
    "#     \"Large CLIP\",\n",
    "#     ]\n",
    "\n",
    "# names_list = [\n",
    "#     \"Baseline\",\n",
    "#     \"Frozen weights\",\n",
    "#     \"Captions\",\n",
    "#     \"Emojis\",\n",
    "#     \"Captions+Emojis\",\n",
    "#     \"Augment (t=0.82)\",\n",
    "#     \"Augment (t=0.84)\",\n",
    "#     ]\n",
    "# newnames = {f\"{s}_study\": names_list[i] for i, s in enumerate(study_name_list)}\n",
    "# fig.for_each_trace(lambda t: t.update(name = newnames[t.name]))\n",
    "show(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials[\"base\"][0].params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "def get_trial_f1_scores(trial):\n",
    "    user_attrs = trial.user_attrs.copy()\n",
    "    no_pred_samples = user_attrs.pop(\"no_prediction_samples\")\n",
    "    f1_scores = {key:user_attrs[key][\"f1-score\"] for key in labels}\n",
    "    for key in user_attrs.keys():\n",
    "        if key not in labels:\n",
    "            f1_scores[key] = user_attrs[key][\"f1-score\"]\n",
    "    return f1_scores\n",
    "\n",
    "def get_no_pred_samples(trial):\n",
    "    user_attrs = trial.user_attrs\n",
    "    return user_attrs[\"no_prediction_samples\"]\n",
    "# get_trial_f1_scores(trials[\"bert\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {name: get_trial_f1_scores(trials[name][0]) for name in study_name_list}\n",
    "f1_scores = pd.DataFrame(f1_scores)\n",
    "# f1_scores[\"support\"] = pd.Series(val_support).astype(int)\n",
    "f1_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_agg_list = [\"macro avg\", \"micro avg\", \"weighted avg\", \"samples avg\"]\n",
    "names = [\"Base CLIP\", \"Jina CLIP\", \"Large CLIP\", \"SigLIP\", \"BLIP-2\"]\n",
    "\n",
    "# names = [\n",
    "#     \"Baseline\",\n",
    "#     \"Frozen weights\",\n",
    "#     \"Captions\",\n",
    "#     \"Emojis\",\n",
    "#     \"Captions+Emojis\",\n",
    "#     \"Augment (t=0.82)\",\n",
    "#     \"Augment (t=0.84)\",\n",
    "#     ]\n",
    "ax = f1_scores[study_name_list].loc[labels + f1_agg_list].plot(kind=\"bar\", figsize=(12,4.5))\n",
    "ax.legend(labels=names)\n",
    "ax.set_xticklabels([l.get_text().capitalize() for l in ax.get_xticklabels()], rotation=45, ha='right');\n",
    "\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f1_scores.to_latex())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f1_scores = {name: get_trial_f1_scores(trials[name][0]) for name in study_name_list}\n",
    "# f1_scores = pd.DataFrame(f1_scores)\n",
    "# # f1_scores[\"support\"] = pd.Series(val_support).astype(int)\n",
    "# f1_scores\n",
    "\n",
    "\n",
    "for name in study_name_list:\n",
    "\n",
    "    trials[name].sort(key = lambda x: x.values[1], reverse=True)\n",
    "\n",
    "tmp = []\n",
    "for i in range(10):\n",
    "    tmp.append(pd.DataFrame({name: get_trial_f1_scores(trials[name][i]) for name in study_name_list}))\n",
    "best_avg_metrics = pd.concat(tmp).groupby(level=0).mean()\n",
    "best_avg_metrics = best_avg_metrics.loc[labels + f1_agg_list]\n",
    "\n",
    "\n",
    "ax = best_avg_metrics.plot(kind=\"bar\", figsize=(16,5), yticks=[x / 10 for x in range(0,11)])\n",
    "ax.legend(labels=names,loc=\"upper right\", fontsize=16)\n",
    "\n",
    "ax.set_xticklabels([l.get_text().capitalize() for l in ax.get_xticklabels()], rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "plt.rc('xtick', labelsize=18)\n",
    "plt.rc('ytick', labelsize=18)\n",
    "ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [\"loss\", \"f1-score\", \"accuracy\"]\n",
    "results = {study: {metric: 0 for metric in metrics} for study in study_name_list}\n",
    "top_n = 1\n",
    "\n",
    "for study in study_name_list:\n",
    "    for i, metric in enumerate(metrics):\n",
    "        trials[study].sort(key = lambda x: x.values[i], reverse=True if metric != \"loss\" else False)\n",
    "        for j in range(top_n):\n",
    "            results[study][metric] += trials[study][j].values[i]\n",
    "        results[study][metric] /= top_n\n",
    "print(pd.DataFrame(results).T.to_latex(float_format=\"%.4f\").replace(\"_\", \" \"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero shot CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "import numpy as np\n",
    "import sklearn.metrics as skm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mode=\"M\"\n",
    "train, _ = MulTweEmoDataset.load(csv_path=\"./dataset/train_MulTweEmo.csv\", mode=mode, drop_something_else=True, force_override=True, test_split=None, seed=123)\n",
    "emotions = MulTweEmoDataset.get_labels()\n",
    "emotions.remove(\"something else\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"zero_shot_predictions\", \"rb\") as f:\n",
    "    predictions = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.1\n",
    "f1_scores = {}\n",
    "models = [\"base\", \"jina\", \"large\"]\n",
    "for i in range(predictions.shape[0]):\n",
    "    tmp = predictions[i] > threshold\n",
    "    count = 0\n",
    "    f1_scores[models[i]] = {}\n",
    "    for sample in tmp:\n",
    "        if 1 not in sample:\n",
    "            count+=1\n",
    "    results = skm.classification_report(list(train[\"labels\"]), tmp, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for emotion in emotions:\n",
    "        f1_scores[models[i]][emotion] = results[emotion][\"f1-score\"] \n",
    "    print(models[i])\n",
    "    display(pd.DataFrame(results).T)\n",
    "    print(count, \"samples with no label\")\n",
    "    print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.3\n",
    "f1_scores = {}\n",
    "models = [\"base\", \"jina\", \"large\"]\n",
    "for i in range(predictions.shape[0]):\n",
    "    tmp = predictions[i] > threshold\n",
    "    count = 0\n",
    "    f1_scores[models[i]] = {}\n",
    "    for sample in tmp:\n",
    "        if 1 not in sample:\n",
    "            count+=1\n",
    "    results = skm.classification_report(list(train[\"labels\"]), tmp, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for emotion in emotions:\n",
    "        f1_scores[models[i]][emotion] = results[emotion][\"f1-score\"] \n",
    "pd.DataFrame(f1_scores).plot(kind=\"bar\", yticks=[x / 10 for x in range(0,11)], figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_list = [x / 10 for x in range(1,6)]\n",
    "f1_scores = {}\n",
    "predictions_index = 1\n",
    "for threshold in threshold_list:\n",
    "    tmp = predictions[predictions_index] > threshold\n",
    "    count = 0\n",
    "    f1_scores[threshold] = {}\n",
    "    for sample in tmp:\n",
    "        if 1 not in sample:\n",
    "            count+=1\n",
    "    results = skm.classification_report(list(train[\"labels\"]), tmp, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for emotion in emotions:\n",
    "        f1_scores[threshold][emotion] = results[emotion][\"f1-score\"] \n",
    "pd.DataFrame(f1_scores).plot(kind=\"bar\", title=f\"{models[predictions_index]} clip\", yticks=[x / 10 for x in range(0,11)], figsize=(10,5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "count = 0\n",
    "for sample in predictions:\n",
    "    if 1 not in sample:\n",
    "        count+=1\n",
    "results = skm.classification_report(list(test[\"labels\"]), predictions, zero_division=0, target_names=emotions)\n",
    "print(results)\n",
    "print(skm.accuracy_score(list(test[\"labels\"]), predictions))\n",
    "# for emotion in emotions:\n",
    "#     f1_scores[emotion] = results[emotion][\"f1-score\"] \n",
    "# display(pd.DataFrame(results).T)\n",
    "print(count, \"samples with no label\")\n",
    "# print(\"\\n\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "for i in range(4):\n",
    "    f1_scores[f\"Prompt {i}\"] = {}\n",
    "    llava_results_path = f\"./zero_shot_results/list/results_{i}.np\"\n",
    "    with open(llava_results_path, \"rb\") as f:\n",
    "        predictions = np.load(f)\n",
    "    results = skm.classification_report(list(test[\"labels\"]), predictions, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for key in results.keys():\n",
    "        f1_scores[f\"Prompt {i}\"][key] = results[key][\"f1-score\"] \n",
    "pd.DataFrame(f1_scores).plot(kind=\"bar\", figsize=(10,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pd.DataFrame(f1_scores).to_latex(float_format=\"%.3f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_results_path = \"./zero_shot_results/list/results_3.np\"\n",
    "with open(llava_results_path, \"rb\") as f:\n",
    "    predictions = np.load(f)\n",
    "\n",
    "f1_scores = {}\n",
    "count = 0\n",
    "for sample in predictions:\n",
    "    if 1 not in sample:\n",
    "        count+=1\n",
    "results = skm.classification_report(list(test[\"labels\"]), predictions, zero_division=0, target_names=emotions)\n",
    "print(results)\n",
    "print(skm.accuracy_score(list(test[\"labels\"]), predictions))\n",
    "\n",
    "print(count, \"samples with no label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = pd.DataFrame(skm.classification_report(list(test[\"labels\"]), predictions, zero_division=0, target_names=emotions, output_dict=True)).T.drop(columns=[\"support\"])\n",
    "results_table.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "def count_labels(dataset):\n",
    "    labels = MulTweEmoDataset.get_labels()\n",
    "    labels\n",
    "    count = {}\n",
    "    for i in labels:\n",
    "        count[i] = 0\n",
    "\n",
    "    for i, row in dataset.iterrows():\n",
    "        for label in labels:\n",
    "            count[label] += 1 if row[label] else 0\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test, _ = MulTweEmoDataset.load(csv_path=\"./dataset/val_MulTweEmo.csv\",mode=\"M\", drop_something_else=True, force_override=True, test_split=None)\n",
    "count_labels(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sklearn.metrics as skm\n",
    "preds = np.random.rand(test.shape[0], 9) > 0.5\n",
    "print(skm.classification_report(test[\"labels\"].to_list(), preds, target_names=MulTweEmoDataset.get_labels(), zero_division=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All ones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.ones((test.shape[0], 9), dtype=int)\n",
    "print(skm.classification_report(test[\"labels\"].to_list(), preds, target_names=MulTweEmoDataset.get_labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All Joy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = np.zeros((test.shape[0], 9), dtype=int)\n",
    "for i in range(preds.shape[0]):\n",
    "    preds[i][4] = 1\n",
    "print(skm.classification_report(test[\"labels\"].to_list(), preds, target_names=MulTweEmoDataset.get_labels()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check number of tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "from libs.model import TweetMERConfig\n",
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "\n",
    "model = \"base\"\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(TweetMERConfig.get_feature_extractor_name(model), trust_remote_code=True)\n",
    "dataset, _ = MulTweEmoDataset.load(csv_path=\"./dataset/train_MulTweEmo.csv\",mode=\"M\", drop_something_else=True, test_split=None, emoji_decoding=False)\n",
    "dataset[\"tweet\"] = dataset[\"tweet\"] + \" \" + dataset[\"caption\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor.tokenizer.model_max_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_inputs = processor(\n",
    "                            text = list(dataset[\"tweet\"]), \n",
    "                            padding=False, \n",
    "                            # truncation=True, \n",
    "                            return_tensors=\"np\"\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "sum = 0\n",
    "min = 1024\n",
    "max = 0\n",
    "for inputs in processed_inputs[\"input_ids\"]:\n",
    "    n_tokens = inputs.shape[0]\n",
    "    if n_tokens > processor.tokenizer.model_max_length:\n",
    "        # print(inputs.shape[0])\n",
    "        if n_tokens < min: min=n_tokens\n",
    "        if n_tokens > max: max=n_tokens\n",
    "        sum += n_tokens\n",
    "        count += 1\n",
    "sum /= count\n",
    "print(\" & \".join(str(x) for x in [min, max, sum, count, count/len(dataset)]), \"\\\\\\\\\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlation between labels for dataset and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cols = MulTweEmoDataset.get_labels()\n",
    "fig = plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(dataset[cols].corr(), annot = True, fmt = '.3f')\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "from libs.utils.ModelWrappers import TweetMERWrapper\n",
    "from datasets import Dataset\n",
    "from libs.model import TweetMERModel\n",
    "import sklearn.metrics as skm\n",
    "\n",
    "class TweetMSAObjective(object):\n",
    "    def __init__(self, clip_version=\"jina\", append_captions:bool=False, process_emojis:bool=False, data_augment:bool=False, mode=\"M\", freeze_weights:bool=False, seed:int=123):\n",
    "        self.train, _ = MulTweEmoDataset.load(csv_path=\"./dataset/train_MulTweEmo.csv\", mode=mode, drop_something_else=True,\n",
    "                                               emoji_decoding=process_emojis, test_split=None, seed=seed)\n",
    "        self.val, _ = MulTweEmoDataset.load(csv_path=\"./dataset/val_MulTweEmo.csv\", mode=mode, drop_something_else=True,\n",
    "                                               emoji_decoding=process_emojis, test_split=None, seed=seed)\n",
    "\n",
    "\n",
    "        if append_captions:\n",
    "            tweet_caption_data = self.train.apply(lambda x: x[\"tweet\"] + \" \" + x[\"caption\"], axis=1)\n",
    "            if data_augment:\n",
    "                tweet_caption_train = self.train.copy()\n",
    "                tweet_caption_train[\"tweet\"] = tweet_caption_data\n",
    "                # caption_train = self.train.copy()\n",
    "                # caption_train[\"tweet\"] = caption_train[\"caption\"]\n",
    "                # self.train = pd.concat(self.train, caption_train)\n",
    "                self.train = pd.concat(self.train, tweet_caption_train)\n",
    "            else:\n",
    "                self.train[\"tweet\"] = tweet_caption_data\n",
    "        #    self.val[\"tweet\"] = self.val.apply(lambda x: x[\"tweet\"] + \" \"  + x[\"caption\"], axis=1)\n",
    "\n",
    "\n",
    "        self.train = Dataset.from_pandas(TweetMERModel.preprocess_dataset(dataset=self.train, model=clip_version, text_column=\"tweet\", label_column=\"labels\"))\n",
    "        self.val = Dataset.from_pandas(TweetMERModel.preprocess_dataset(dataset=self.val, model=clip_version, text_column=\"tweet\", label_column=\"labels\"))\n",
    "        self.clip_version = clip_version\n",
    "        self.freeze_weights = freeze_weights\n",
    "\n",
    "    def __call__(self):\n",
    "        model = TweetMERWrapper(n_epochs=3, warmup_steps=30, learning_rate=6.263149136769504e-05, \n",
    "                                 batch_size=16, n_layers=4, n_units=76,\n",
    "                                 dropout=0.16, clip_version=self.clip_version, freeze_weights=self.freeze_weights)\n",
    "        model.fit(self.train, self.train[\"labels\"])\n",
    "        predictions, results =  model.score(self.val, self.val[\"labels\"])\n",
    "        label_names = MulTweEmoDataset.get_labels()\n",
    "        metrics = skm.classification_report(self.val[\"labels\"], predictions, output_dict=True, zero_division=0, target_names=label_names)\n",
    "        count = 0\n",
    "        for sample in predictions:\n",
    "            if 1 not in sample:\n",
    "                count+=1\n",
    "        del model\n",
    "        return count, results[\"loss\"], results[\"f1_score\"], results[\"exact_match\"], metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = TweetMSAObjective(clip_version=\"base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from libs.dataset_loader import MulTweEmoDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, _ = MulTweEmoDataset.load(csv_path=\"./dataset/new_split.bak/train_MulTweEmo.csv\", mode=\"M\", drop_something_else=True, test_split=None, seed=123)\n",
    "val, _ = MulTweEmoDataset.load(csv_path=\"./dataset/new_split.bak/val_MulTweEmo.csv\", mode=\"M\", drop_something_else=True, test_split=None, seed=123)\n",
    "test, _ = MulTweEmoDataset.load(csv_path=\"./dataset/new_split.bak/test_MulTweEmo.csv\", mode=\"M\", drop_something_else=True, test_split=None, seed=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "train_id = train.id.values\n",
    "for value in test.id.values:\n",
    "    if value in train_id:\n",
    "        count += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val[\"id\"].isin(train_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = val[val[\"id\"].isin(train_id)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = MulTweEmoDataset.get_labels()\n",
    "count = {label: 0 for label in labels}\n",
    "for i, row in tmp.iterrows():\n",
    "    for label in labels:\n",
    "        if row[label]:\n",
    "            count[label] += 1\n",
    "count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaky ReLU plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.vectorize(relu)([3.2, 2.1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x.copy()\n",
    "y[y<0] = 0\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "def leaky_relu(x, alpha=0.1):\n",
    "    return max(alpha*x, x)\n",
    "\n",
    "def relu(x:float):\n",
    "    if x > 0:\n",
    "        return x\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "sigmoid = torch.nn.functional.sigmoid\n",
    "\n",
    "x = np.linspace(-21, 21, 1000)\n",
    "\n",
    "# y = np.vectorize(leaky_relu)(x)\n",
    "y = x.copy()\n",
    "y[y<0] = 0\n",
    "# y = sigmoid(torch.tensor(x))\n",
    "\n",
    "fig = plt.figure(figsize=(5,3))\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlim(-20, 20)\n",
    "\n",
    "ax.set_xlabel(\"Input\")\n",
    "ax.set_ylabel(\"Output\")\n",
    "plt.grid()\n",
    "# ax.set_aspect(\"equal\")\n",
    "ax.axhline(0, color='#777777')\n",
    "ax.axvline(0, color='#777777')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid(torch.tensor(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def leaky_relu(x, alpha=0.1):\n",
    "    return max(alpha*x, x)\n",
    "\n",
    "def relu(x):\n",
    "    return max(0, x)\n",
    "\n",
    "x = np.linspace(-21, 21, 1000)\n",
    "y = np.vectorize(relu)(x)\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca()\n",
    "\n",
    "ax.set_xlim(-20, 20)\n",
    "\n",
    "ax.set_xlabel(\"Input\")\n",
    "ax.set_ylabel(\"Output\")\n",
    "plt.grid()\n",
    "ax.set_aspect(\"equal\")\n",
    "ax.axhline(0, color='#777777')\n",
    "ax.axvline(0, color='#777777')\n",
    "plt.plot(x, y)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
