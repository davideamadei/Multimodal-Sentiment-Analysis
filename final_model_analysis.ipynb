{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from libs.dataset_loader import MulTweEmoDataset\n",
    "import sklearn.metrics as skm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import optuna\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_predictions(data, threshold):\n",
    "    y_pred = np.zeros(data.shape)\n",
    "    for i, pred in enumerate(data):\n",
    "        y_pred[i] = pred > threshold\n",
    "    return y_pred\n",
    "\n",
    "def get_metrics(labels, data, target_names):\n",
    "    results = skm.classification_report(labels, data, output_dict=True, zero_division=0, target_names=target_names)\n",
    "    results = pd.DataFrame(results)\n",
    "    results.columns = map(str.capitalize, results.columns)\n",
    "    results = results.T.drop(columns=\"support\")\n",
    "    results.columns = map(str.capitalize, results.columns)\n",
    "    return results\n",
    "\n",
    "def plot_metrics(labels, data, target_names):\n",
    "    results = get_metrics(labels, data, target_names)\n",
    "    ax = pd.DataFrame(results).plot(kind=\"bar\", figsize=(10,4), yticks=[x / 10 for x in range(0,11)])\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True)\n",
    "    \n",
    "def metrics_to_latex(labels, data, target_names):\n",
    "    results = get_metrics(labels, data, target_names)\n",
    "    return(results.to_latex(float_format=\"%.4f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_type = \"base\"\n",
    "datasets = [\"train\", \"val\", \"test\"]\n",
    "predictions = {}\n",
    "\n",
    "classes = list(range(9))\n",
    "drop_low_support=False\n",
    "if model_type == \"high_support\":\n",
    "    classes = [0,1,2,4,5,6]\n",
    "    drop_low_support=True\n",
    "\n",
    "load_dir = \"./multimodal_results\"\n",
    "\n",
    "val, _ = MulTweEmoDataset.load(csv_path=\"./dataset/val_MulTweEmo.csv\", drop_something_else=True, drop_low_support=drop_low_support, test_split=None)\n",
    "test, _ = MulTweEmoDataset.load(csv_path=\"./dataset/test_MulTweEmo.csv\", drop_something_else=True, drop_low_support=drop_low_support, test_split=None)\n",
    "train, _ =  MulTweEmoDataset.load(csv_path=\"./dataset/train_MulTweEmo.csv\", drop_something_else=True, drop_low_support=drop_low_support, test_split=None)\n",
    "for set in datasets:\n",
    "\n",
    "    with open(f\"{load_dir}/{model_type}/{set}_predictions.np\", \"rb\") as f:\n",
    "        predictions[set] = np.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "storage_name = \"sqlite:///final_study_2.db\"\n",
    "final_model_trial = {\"bert\": 268,\n",
    "                     \"base\":260,\n",
    "                     \"base_captions\":287,\n",
    "                     \"base_augment\":214,\n",
    "                     \"high_support\":187,\n",
    "                     \"text_only\":148}\n",
    "study = optuna.create_study(study_name=model_type+\"_final_study\", storage=storage_name, load_if_exists=True, directions=[\"minimize\", \"maximize\", \"maximize\"])\n",
    "trials = study.get_trials()\n",
    "params = trials[final_model_trial[model_type]].params\n",
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def format_number(number):\n",
    "#     if type(number) is int:\n",
    "#         return str(number)\n",
    "#     else:\n",
    "#         return \"{:.6f}\".format(number)\n",
    "print(\"\\\\begin{tabular}{|\" + \"r|\"*len(params.keys()) + \"}\\n\\\\hline\")\n",
    "print(\" & \".join([key.replace(\"_\", \"\\_\") for key in params.keys()]) + \"\\\\\\\\\\n\\\\hline\")\n",
    "print(\" & \".join([str(value) for value in params.values()]) + \"\\\\\\\\\\n\\\\hline\")\n",
    "print(\"\\\\end{tabular}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "  return 1 / (1 + math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions = np.array(predictions[\"val\"])\n",
    "test_predictions = np.array(predictions[\"test\"])\n",
    "train_predictions = np.array(predictions[\"train\"])\n",
    "emotions = MulTweEmoDataset.get_labels(drop_low_support=drop_low_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_type == \"bert\":\n",
    "    test = test.drop_duplicates(subset=[\"id\"])\n",
    "    val = val.drop_duplicates(subset=[\"id\"])\n",
    "    train = train.drop_duplicates(subset=[\"id\"])\n",
    "    test_predictions = test_predictions[test.index]\n",
    "    val_predictions = np.vectorize(sigmoid)(val_predictions)\n",
    "    test_predictions = np.vectorize(sigmoid)(test_predictions)\n",
    "    train_predictions = np.vectorize(sigmoid)(train_predictions)\n",
    "if model_type == \"text_only\":\n",
    "    test = test.drop_duplicates(subset=[\"id\"])\n",
    "    val = val.drop_duplicates(subset=[\"id\"])\n",
    "    train = train.drop_duplicates(subset=[\"id\"])\n",
    "if model_type == \"base_augment\":\n",
    "    train_predictions = train_predictions[:train.shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_labels = np.array(val[\"labels\"].to_list())\n",
    "test_labels = np.array(test[\"labels\"].to_list())\n",
    "train_labels = np.array(train[\"labels\"].to_list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation on validation and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_default = threshold_predictions(val_predictions, 0.5)\n",
    "test_default = threshold_predictions(test_predictions, 0.5)\n",
    "train_default = threshold_predictions(train_predictions, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.accuracy_score(val_labels, threshold_predictions(val_predictions, 0.5)))\n",
    "get_metrics(val_labels, val_default, target_names=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(val_labels, val_default, target_names=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_to_latex(val_labels, val_default, emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.accuracy_score(test_labels, threshold_predictions(test_predictions, 0.5)))\n",
    "get_metrics(test_labels, test_default, target_names=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supports = np.array(test_labels).sum(axis=0).astype(int)\n",
    "test_supports = np.append(test_supports,[test_supports.sum()]*4)\n",
    "default_threshold_results = get_metrics(test_labels, test_default, target_names=emotions)\n",
    "default_threshold_results.columns = default_threshold_results.columns.map(lambda x: x+\" \")\n",
    "default_threshold_results[\"Support\"] = test_supports\n",
    "print(default_threshold_results.to_latex(float_format=\"%.4f\", column_format=\"l|rrr|r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_losses = {\n",
    "\"bert\": 0.38037171959877014,\n",
    "\"base\": 0.3544072210788727,\n",
    "\"base_captions\": 0.37323105335235596,\n",
    "\"base_augment\": 0.4171484708786011,\n",
    "\"high_support\": 0.42006585001945496,\n",
    "\"text_only\": 0.3724941909313202}\n",
    "\n",
    "metrics = ['loss', 'exact_match', 'precision', 'recall', 'f1_score']\n",
    "\n",
    "with open(f\"checkpoint_metrics/{model_type}.json\", \"r\") as fp:\n",
    "    training_results = json.load(fp)\n",
    "summary_results = {\"train\": {}, \"val\": {}, \"test\": {}}\n",
    "\n",
    "summary_results[\"train\"] = training_results[\"train\"][list(training_results[\"train\"].keys())[-1]]\n",
    "summary_results[\"val\"] = training_results[\"val\"][list(training_results[\"val\"].keys())[-1]]\n",
    "\n",
    "# summary_results[\"val\"][\"loss\"] =  training_results[\"val\"][list(training_results[\"val\"].keys())[-1]][\"loss\"]\n",
    "# summary_results[\"val\"][\"exact_match\"] =  skm.accuracy_score(val_labels, val_default)\n",
    "# summary_results[\"val\"][\"precision\"] =  skm.precision_score(val_labels, val_default, zero_division=0, average=\"samples\")\n",
    "# summary_results[\"val\"][\"recall\"] =  skm.recall_score(val_labels, val_default, zero_division=0, average=\"samples\")\n",
    "# summary_results[\"val\"][\"f1_score\"] =  skm.f1_score(val_labels, val_default, zero_division=0, average=\"samples\")\n",
    "\n",
    "summary_results[\"test\"][\"loss\"] = test_losses[model_type]\n",
    "summary_results[\"test\"][\"exact_match\"] = skm.accuracy_score(test_labels, threshold_predictions(test_predictions, 0.5))\n",
    "summary_results[\"test\"][\"precision\"] =  skm.precision_score(test_labels, test_default, zero_division=0, average=\"samples\")\n",
    "summary_results[\"test\"][\"recall\"] =  skm.recall_score(test_labels, test_default, zero_division=0, average=\"samples\")\n",
    "summary_results[\"test\"][\"f1_score\"] =  skm.f1_score(test_labels, test_default, zero_division=0, average=\"samples\")\n",
    "\n",
    "summary_results = pd.DataFrame(summary_results)\n",
    "summary_results.columns = [\"Training\", \"Validation\", \"Test\"]\n",
    "summary_results = summary_results.T[metrics]\n",
    "cols = [\"Loss\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-score\"]\n",
    "summary_results.columns = cols\n",
    "print(summary_results.to_latex(float_format=\"%.4f\", column_format=\"l|rrrrr\"))\n",
    "summary_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(test_labels, test_default, target_names=emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(metrics_to_latex(test_labels, test_default, emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_default.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_predictions(np.array(test_labels), 0.5).sum(axis=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(12, 7))\n",
    "sns.heatmap(pd.DataFrame(threshold_predictions(test_predictions, 0.5)).corr(), annot = True, fmt = '.3f', xticklabels=emotions, yticklabels=emotions, vmin=-1, vmax=1)\n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Threshold analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(10,5)).clf()\n",
    "\n",
    "for i, emotion in enumerate(emotions):\n",
    "    index = 3\n",
    "    precision, recall, thresholds = skm.precision_recall_curve(train_labels[:, i], train_predictions[:, i])\n",
    "    plt.plot(precision,recall,label=f\"{emotion.capitalize()}\")\n",
    "\n",
    "plt.xlabel(\"recall\")\n",
    "plt.ylabel(\"precision\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(0, figsize=(10,5)).clf()\n",
    "\n",
    "\n",
    "for i, emotion in enumerate(emotions):\n",
    "    index = 3\n",
    "    fpr, tpr, thresholds = skm.roc_curve(test_labels[:,i], test_predictions[:,i])\n",
    "    auc = skm.auc(fpr, tpr)\n",
    "    plt.plot(fpr,tpr,label=f\"{emotion.capitalize()}, auc=\"+\"{:.4f}\".format(auc))\n",
    "# print(fpr, tpr, thresholds)\n",
    "\n",
    "plt.xlabel(\"FPR\")\n",
    "plt.ylabel(\"TPR\")\n",
    "plt.legend(loc=0)\n",
    "plt.show()\n",
    "# display = skm.RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=roc_auc,\n",
    "\n",
    "#                                 estimator_name='example estimator')\n",
    "# display.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective(object):\n",
    "    def __init__(self, train_logits, train_labels, val_logits, val_labels, base_prec, base_rec, ranges):\n",
    "        self.train_logits = train_logits.copy()\n",
    "        self.train_labels = train_labels\n",
    "        self.val_logits = val_logits.copy()\n",
    "        self.val_labels = val_labels\n",
    "        self.base_prec = base_prec\n",
    "        self.base_rec = base_rec\n",
    "        self.ranges = ranges\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        emotions = MulTweEmoDataset.get_labels(drop_low_support=drop_low_support)\n",
    "        thresholds = [trial.suggest_float(emotion, self.ranges[emotion][0], self.ranges[emotion][1]) for emotion in emotions]\n",
    "        preds = np.zeros(self.train_logits.shape)\n",
    "        for i, pred in enumerate(self.train_logits):\n",
    "            preds[i] = pred > thresholds\n",
    "        \n",
    "\n",
    "        train_precision = skm.precision_score(self.train_labels, preds, zero_division=0, average=\"samples\")\n",
    "        train_recall = skm.recall_score(self.train_labels, preds, zero_division=0, average=\"samples\")\n",
    "        \n",
    "        val_preds = np.zeros(self.val_logits.shape)\n",
    "        for i, pred in enumerate(self.val_logits):\n",
    "            val_preds[i] = pred > thresholds\n",
    "        val_precision = skm.precision_score(self.val_labels, val_preds, zero_division=0, average=\"samples\")\n",
    "        val_recall = skm.recall_score(self.val_labels, val_preds, zero_division=0, average=\"samples\")\n",
    "        \n",
    "        metrics = skm.classification_report(self.val_labels, val_preds, output_dict=True, zero_division=0, target_names=emotions)\n",
    "\n",
    "        if val_precision < self.base_prec:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        if val_recall < self.base_rec:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        \n",
    "        trial.set_user_attr(\"Precision\", val_precision)\n",
    "        trial.set_user_attr(\"Recall\", val_recall)\n",
    "\n",
    "        for key, value in metrics.items():\n",
    "            trial.set_user_attr(key, value)\n",
    "        count = 0\n",
    "        for sample in preds:\n",
    "            if 1 not in sample:\n",
    "                count+=1\n",
    "                \n",
    "        trial.set_user_attr(\"no_prediction_samples\", count)\n",
    "\n",
    "        return train_precision, train_recall\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranges = {e: [0.15, 0.6] for e in emotions}\n",
    "\n",
    "val_metrics = get_metrics(val_labels, val_default, target_names=emotions)\n",
    "\n",
    "objective = Objective(train_predictions, train_labels, val_predictions, val_labels,\n",
    "                       base_prec=val_metrics[\"Precision\"][\"Samples avg\"], base_rec=val_metrics[\"Recall\"][\"Samples avg\"], ranges=ranges)\n",
    "storage_name = f\"sqlite:///threshold_training_study.db\"\n",
    "study = optuna.create_study(study_name=f\"{model_type}\", storage=storage_name, load_if_exists=True, directions=[\"maximize\", \"maximize\"])\n",
    "study.set_metric_names([\"precision\", \"recall\"])\n",
    "\n",
    "if len(study.trials) == 0:\n",
    "    study.optimize(objective,n_trials=500)\n",
    "trials = study.get_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective_val(object):\n",
    "    def __init__(self, trials):\n",
    "        self.trials = trials\n",
    "\n",
    "    def __call__(self, trial):\n",
    "        index=trial.number\n",
    "        if trials[index].state == 2:\n",
    "            raise optuna.exceptions.TrialPruned()\n",
    "        precision = self.trials[index].user_attrs[\"Precision\"]\n",
    "        recall = self.trials[index].user_attrs[\"Recall\"]\n",
    "        return precision, recall\n",
    "    \n",
    "objective = Objective_val(trials)\n",
    "storage_name = f\"sqlite:///threshold_val_study.db\"\n",
    "study_val = optuna.create_study(study_name=f\"{model_type}\", storage=storage_name, load_if_exists=True, directions=[\"maximize\", \"maximize\"])\n",
    "study_val.set_metric_names([\"precision\", \"recall\"])\n",
    "\n",
    "if len(study_val.trials) == 0:\n",
    "    study_val.optimize(objective,n_trials=500)\n",
    "trials = study_val.get_trials()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_pareto_front(study=study_val, targets=lambda x:(x.values[0], x.values[1]), target_names=[\"Precision\", \"Recall\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=400,\n",
    "    title=None,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font_size=14,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_treshold_trials = {\n",
    "    \"bert\":497,\n",
    "    \"base\":323,\n",
    "    \"base_captions\":158,\n",
    "    \"base_augment\":474,\n",
    "    \"high_support\":184,\n",
    "    \"text_only\": 414\n",
    "}\n",
    "trial = study.trials[best_treshold_trials[model_type]]\n",
    "thresholds = [t for t in trial.params.values()]\n",
    "# thresholds = [thresholds[i] for i in classes]\n",
    "thresholds[0]=0.09033203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = optuna.visualization.plot_pareto_front(study=study_val, targets=lambda x:(x.values[0], x.values[1]), target_names=[\"Precision\", \"Recall\"])\n",
    "\n",
    "fig.update_layout(\n",
    "    autosize=False,\n",
    "    width=1100,\n",
    "    height=400,\n",
    "    title=None,\n",
    "    margin=dict(l=20, r=20, t=20, b=20),\n",
    "    font_size=14,\n",
    ")\n",
    "fig.add_scatter(x=[trial.user_attrs[\"Precision\"]], y=[trial.user_attrs[\"Recall\"]], marker_size=12, marker_symbol=\"star\", marker_color=\"Yellow\", \n",
    "                marker_line_width=1, marker_line_color=\"black\", showlegend=False)\n",
    "fig.write_image(f\"final_models/{model_type}/pareto_front.png\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\\\begin{table}\\n\\t\\\\centering\")\n",
    "print(\"\\t\\\\begin{adjustbox}{width=\\\\textwidth,center=\\\\textwidth}\")\n",
    "print(\"\\t\\\\begin{tabular}{l|\" + \"r\"*len(emotions) + \"}\")\n",
    "print(\"\\t\\t\\\\toprule\")\n",
    "print(\"\\t\\tEmotion & \" + \" & \".join([e.capitalize() for e in emotions]) + \"\\\\\\\\ \\n\\t\\t\\\\midrule\")\n",
    "print(\"\\t\\tThreshold & \" + \" & \".join([str(round(t, 5)) for t in thresholds]) + \"\\\\\\\\\")\n",
    "# print(\" & \".join([key.replace(\"_\", \"\\_\") for key in params.keys()]) + \"\\\\\\\\\\n\\\\hline\")\n",
    "# print(\" & \".join([str(value) for value in params.values()]) + \"\\\\\\\\\\n\\\\hline\")\n",
    "print(\"\\t\\t\\\\bottomrule\")\n",
    "print(\"\\t\\\\end{tabular}\")\n",
    "print(\"\\t\\\\end{adjustbox}\")\n",
    "print(f\"\\t\\\\caption{{() Optimal thresholds}}\\n\\t\\\\label{{tab:{model_type}_thresholds}}\")\n",
    "print(\"\\\\end{table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_custom = threshold_predictions(val_predictions, thresholds)\n",
    "test_custom =  threshold_predictions(test_predictions, thresholds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.accuracy_score(val_labels, val_custom))\n",
    "unique, counts = np.unique((val_custom).sum(axis=1), return_counts=True)\n",
    "print(0 if unique[0]!=0 else counts[0])\n",
    "get_metrics(val_labels, val_custom, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_custom.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(skm.accuracy_score(test_labels, test_custom))\n",
    "unique, counts = np.unique((test_custom).sum(axis=1), return_counts=True)\n",
    "print(0 if unique[0]!=0 else counts[0])\n",
    "get_metrics(test_labels, test_custom, emotions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([e.capitalize() for e in emotions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(labels, data, target_names):\n",
    "    results = get_metrics(labels, data, target_names).loc[[e.capitalize() for e in target_names]]\n",
    "    print(results.loc[[e.capitalize() for e in target_names]])\n",
    "    ax = pd.DataFrame(results).plot(kind=\"bar\", figsize=(6,4), yticks=[x / 10 for x in range(0,11)])\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');\n",
    "    ax.set_axisbelow(True)\n",
    "    ax.yaxis.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rc('xtick', labelsize=14)\n",
    "plt.rc('ytick', labelsize=14)\n",
    "plot_metrics(test_labels, test_custom, emotions)\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_threshold_results = get_metrics(test_labels, test_default, emotions)\n",
    "\n",
    "custom_threshold_results = get_metrics(test_labels, test_custom, emotions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_threshold_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = ['Precision', 'Recall', 'F1-score']\n",
    "comparison_dict = {key:{} for key in default_threshold_results.T.keys()}\n",
    "for e in comparison_dict.keys():\n",
    "    for metric in metrics:\n",
    "        comparison_dict[e][f\"{metric}\".capitalize() + \" (Default)\"] = default_threshold_results[metric][e]\n",
    "        comparison_dict[e][f\"{metric}\".capitalize() + \" (Optimized)\"] = custom_threshold_results[metric][e]\n",
    "my_colors = [(0.5,0.4,0.5), (0.75, 0.75, 0.25)]*5\n",
    "ax = pd.DataFrame(comparison_dict).T.plot(kind='bar', figsize=(13,4.5),\n",
    "                                           color=[\"#ff0000\", \"#990000\", \"#0066ff\", \"#003399\", \"#33cc33\", \"#196619\"], yticks=[x / 10 for x in range(0,11)])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True)\n",
    "\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# # Put a legend to the right of the current axis\n",
    "# ax.legend(loc='lower left', bbox_to_anchor=(1, 0.62))\n",
    "ax.legend(loc='upper right')\n",
    "plt.savefig(f\"final_models/{model_type}/emotion_threshold_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Number of samples in test set with no label assigned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique, counts = np.unique(test_default.sum(axis=1), return_counts=True)\n",
    "print(0 if unique[0]!=0 else counts[0])\n",
    "unique, counts = np.unique(test_custom.sum(axis=1), return_counts=True)\n",
    "print(0 if unique[0]!=0 else counts[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_default.sum(axis=1).mean())\n",
    "print(test_custom.sum(axis=1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supports = np.array(test_labels).sum(axis=0).astype(int)\n",
    "test_supports = np.append(test_supports,[test_supports.sum()]*4)\n",
    "default_threshold_results.columns = default_threshold_results.columns.map(lambda x: x+\" \")\n",
    "joined_results = pd.concat([default_threshold_results, custom_threshold_results], axis=1, join=\"inner\")\n",
    "joined_results[\"Support\"] = test_supports\n",
    "print(joined_results.to_latex(float_format=\"%.4f\", column_format=\"l|rrr|rrr|r\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(test_custom).corr()-pd.DataFrame(test_labels).corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(pd.DataFrame(test_custom).corr(), annot = True, fmt = '.3f',\n",
    "             xticklabels=[e.capitalize() for e in emotions], yticklabels=[e.capitalize() for e in emotions],\n",
    "             square=True)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(pd.DataFrame(test_labels).corr(), annot = True, fmt = '.3f',\n",
    "             xticklabels=[e.capitalize() for e in emotions], yticklabels=[e.capitalize() for e in emotions],\n",
    "             square=True)\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.show()\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(skm.classification_report(test_labels, test_predictions>0.5, target_names=emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_classes=[1,4,7,8]\n",
    "neg_classes=[0,2,3,6]\n",
    "# pos_classes=[1,2,3,4,6,7,8]\n",
    "# neg_classes=[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kek=0\n",
    "for test in test_custom:\n",
    "    for i in range(len(test)):\n",
    "        if test[i]:\n",
    "            for j in range(len(test)):\n",
    "                if test[j]:\n",
    "                    if i in pos_classes and j in neg_classes:\n",
    "                        kek+=1\n",
    "                        # print(test)\n",
    "kek"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(labels, predictions, class_names, supports, title=\"Confusion Matrix\", save_name=None):\n",
    "    num_classes = len(class_names)\n",
    "    num_samples = labels.shape[0]\n",
    "    confusion_mtx = np.zeros((num_classes, num_classes), dtype=float)\n",
    "    npl = np.zeros(num_classes)\n",
    "\n",
    "    for i in range(num_samples):\n",
    "        true_labels = np.where(labels[i] == 1)[0]\n",
    "        pred_labels = np.where(predictions[i] == 1)[0]\n",
    "        \n",
    "        \n",
    "        for t in true_labels:\n",
    "            # cat 1\n",
    "            if t not in pred_labels:\n",
    "                npl[t] += 1\n",
    "            \n",
    "            if t in pred_labels:\n",
    "                confusion_mtx[t, t] += 1\n",
    "            for p in pred_labels:\n",
    "                if p not in true_labels:\n",
    "                    confusion_mtx[t, p] += 1\n",
    "            #         # print(t,p)\n",
    "                # if t not in pred_labels and p not in true_labels:\n",
    "                #     confusion_mtx[t, p] += 1\n",
    "            #         # print(t, p)\n",
    "\n",
    "    for i in range(confusion_mtx.shape[0]):\n",
    "        confusion_mtx[i] = confusion_mtx[i]/(supports[i])\n",
    "\n",
    "    def plot_confusion_matrix(cm, class_names):\n",
    "        fig, ax = plt.subplots(figsize=(7,7))\n",
    "        im = ax.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "\n",
    "        cax = fig.add_axes([ax.get_position().x1+0.01,ax.get_position().y0,0.02,ax.get_position().height])\n",
    "        ax.figure.colorbar(im, cax=cax)\n",
    "        \n",
    "        ax.set(\n",
    "            xticks=np.arange(cm.shape[1]),\n",
    "            yticks=np.arange(cm.shape[0]),\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            xlabel='Predicted Label',\n",
    "            ylabel='True Label',\n",
    "            title=title,\n",
    "        )\n",
    "        plt.rc('font', size=11)\n",
    "        plt.setp(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "        thresh = cm.max() / 2\n",
    "        for i in range(cm.shape[0]):\n",
    "            for j in range(cm.shape[1]):\n",
    "                ax.text(\n",
    "                    j, i,  \"{:.3f}\".format(cm[i, j]),\n",
    "                    # j, i,  int(cm[i, j]),\n",
    "                    ha='center', va='center',\n",
    "                    color='white' if cm[i, j] > thresh else 'black', size=11\n",
    "                )\n",
    "        # plt.tight_layout()\n",
    "        if save_name:\n",
    "            plt.savefig(save_name, dpi=300, bbox_inches='tight')\n",
    "        plt.show()\n",
    "    print(npl)\n",
    "    plot_confusion_matrix(confusion_mtx, class_names)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supports = np.array(test_labels).sum(axis=0)\n",
    "test_supports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(np.array(test_labels), test_default, [e.capitalize() for e in emotions],\n",
    "                  test_supports, \"Default thresholds\", save_name=f\"final_models/{model_type}/conf_matrix_default.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(np.array(test_labels), test_custom, [e.capitalize() for e in emotions],\n",
    "                  test_supports, \"Optimized thresholds\", save_name=f\"final_models/{model_type}/conf_matrix_custom.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "true_example = np.array([[0, 1, 0],\n",
    "                [1, 0, 1],\n",
    "                [1, 0, 0],\n",
    "                [0, 1, 1]])\n",
    "pred_example = np.array([[1, 1, 0],\n",
    "                [0, 1, 1],\n",
    "                [1, 0, 0],\n",
    "                [0, 0, 1]])\n",
    "confusion_matrix(true_example, pred_example, class_names=[0,1,2], supports=pred_example.sum(axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skm.multilabel_confusion_matrix(test_labels, test_custom)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "# f, axes = plt.subplots(2, 5, figsize=(15, 10))\n",
    "# axes = axes.ravel()\n",
    "# for i, e in enumerate(emotions):\n",
    "#     disp = ConfusionMatrixDisplay(confusion_matrix(np.array(test_labels)[:, i],\n",
    "#                                                    test_custom[:, i]),\n",
    "#                                   display_labels=[0, 1])\n",
    "#     disp.plot(ax=axes[i])\n",
    "#     disp.ax_.set_title(e.capitalize())\n",
    "#     if i<5:\n",
    "#         disp.ax_.set_xlabel('')\n",
    "#     if i%5!=0:\n",
    "#         disp.ax_.set_ylabel('')\n",
    "#     disp.im_.colorbar.remove()\n",
    "# plt.subplots_adjust(bottom=0.17, top=0.7, hspace=0.1)\n",
    "# # f.colorbar(disp.im_, ax=axes)\n",
    "# # plt.tight_layout()\n",
    "# f.delaxes(axes[-1])\n",
    "# # plt.rcParams.update({'font.size': 22})\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Zero-shot LLaVA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val, _ = MulTweEmoDataset.load(csv_path=\"./dataset/val_MulTweEmo.csv\", drop_something_else=True, test_split=None)\n",
    "test, _ = MulTweEmoDataset.load(csv_path=\"./dataset/test_MulTweEmo.csv\", drop_something_else=True, test_split=None)\n",
    "val_labels = val[\"labels\"].to_list()\n",
    "test_labels = test[\"labels\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_names = [\"Base\",\n",
    "                \"Context\",\n",
    "                \"Posting\",\n",
    "                \"Expert\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "for i in range(4):\n",
    "    f1_scores[f\"Prompt {i}\"] = {}\n",
    "    llava_results_path = f\"./zero_shot_results/list/results_{i}.np\"\n",
    "    with open(llava_results_path, \"rb\") as f:\n",
    "        val_predictions = np.load(f)\n",
    "    results = skm.classification_report(val_labels, val_predictions, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for key in results.keys():\n",
    "        f1_scores[f\"Prompt {i}\"][key.capitalize()] = results[key][\"f1-score\"] \n",
    "    print(f\"Average number of labels: {val_predictions.sum(axis=1).mean()}\")\n",
    "    print(skm.accuracy_score(val_labels, val_predictions))\n",
    "ax = pd.DataFrame(f1_scores).plot(kind=\"bar\", figsize=(7,4), yticks=[x / 10 for x in range(0,11)])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True)\n",
    "ax.legend(prompt_names)\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# # Put a legend to the right of the current axis\n",
    "# ax.legend(loc='lower left', bbox_to_anchor=(1, 0.7))\n",
    "plt.savefig(f\"final_models/llava/list_prompts_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_scores = {}\n",
    "for i in range(4):\n",
    "    f1_scores[f\"Prompt {i}\"] = {}\n",
    "    llava_results_path = f\"./zero_shot_results/binary/results_{i}.np\"\n",
    "    with open(llava_results_path, \"rb\") as f:\n",
    "        val_predictions = np.load(f)\n",
    "    results = skm.classification_report(val_labels, val_predictions, zero_division=0, target_names=emotions, output_dict=True)\n",
    "    for key in results.keys():\n",
    "        f1_scores[f\"Prompt {i}\"][key.capitalize()] = results[key][\"f1-score\"] \n",
    "    print(f\"Average number of labels: {val_predictions.sum(axis=1).mean()}\")\n",
    "    print(skm.accuracy_score(val_labels, val_predictions))\n",
    "ax = pd.DataFrame(f1_scores).plot(kind=\"bar\", figsize=(7,4), yticks=[x / 10 for x in range(0,11)])\n",
    "ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right');\n",
    "ax.set_axisbelow(True)\n",
    "ax.yaxis.grid(True)\n",
    "ax.legend(prompt_names)\n",
    "\n",
    "# box = ax.get_position()\n",
    "# ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
    "# # Put a legend to the right of the current axis\n",
    "# ax.legend(loc='lower left', bbox_to_anchor=(1, 0.7))\n",
    "plt.savefig(f\"final_models/llava/binary_prompts_comparison.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_table = {}\n",
    "for j in range(1, 3):\n",
    "    results_table[\"Method \"+str(j)] = {}\n",
    "    for i in range(4):\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)] = {}\n",
    "        if j == 1:\n",
    "            llava_results_path = f\"./zero_shot_results/binary/results_{i}.np\"\n",
    "        else:\n",
    "            llava_results_path = f\"./zero_shot_results/list/results_{i}.np\"\n",
    "        with open(llava_results_path, \"rb\") as f:\n",
    "            val_predictions = np.load(f)\n",
    "            \n",
    "        model_results = get_metrics(val_labels, val_predictions, emotions)\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"Precision\"] = model_results[\"Precision\"][\"Samples avg\"]\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"Recall\"] = model_results[\"Recall\"][\"Samples avg\"]\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"F1-score\"] = model_results[\"F1-score\"][\"Samples avg\"]\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"F1-score\"]\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"Accuracy\"] = skm.accuracy_score(val_labels, val_predictions,)\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"Hamming\"] = skm.hamming_loss(val_labels, val_predictions,)\n",
    "        \n",
    "        unique, count = np.unique(val_predictions.sum(axis=1), return_counts=True)\n",
    "        print(unique,count)\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"No labels\"] = 0 if unique[0]!=0 else count[0]\n",
    "        results_table[\"Method \"+str(j)][\"Prompt \"+str(i)][\"Average labels\"] = val_predictions.sum(axis=1).mean()\n",
    "\n",
    "results_table_1 = pd.DataFrame(results_table[\"Method 1\"]).T\n",
    "results_table_2 = pd.DataFrame(results_table[\"Method 2\"]).T\n",
    "results_table_1[\"No labels\"] = results_table_1[\"No labels\"].astype(int)\n",
    "results_table_2[\"No labels\"] = results_table_2[\"No labels\"].astype(int)\n",
    "print(results_table_1.to_latex(float_format=\"%.4f\", column_format=\"l|rrr|rr|rr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llava_results_path = \"./zero_shot_results/test/results_3.np\"\n",
    "with open(llava_results_path, \"rb\") as f:\n",
    "    test_predictions = np.load(f)\n",
    "\n",
    "unique, count = np.unique(test_predictions.sum(axis=1), return_counts=True)\n",
    "count = count[0]\n",
    "print(f\"Average number of labels: {test_predictions.sum(axis=1).mean()}\")\n",
    "print(skm.accuracy_score(test_labels, test_predictions))\n",
    "\n",
    "print(count, \"samples with no label\\n\\n\")\n",
    "print(metrics_to_latex(test_labels, test_predictions, emotions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./zero_shot_results/list/results_3.np\", \"rb\") as f:\n",
    "    val_predictions = np.load(f)\n",
    "\n",
    "with open(\"./zero_shot_results/test/results_3.np\", \"rb\") as f:\n",
    "    test_predictions = np.load(f)\n",
    "\n",
    "val_results = get_metrics(val_labels, val_predictions, emotions)\n",
    "test_results = get_metrics(test_labels, test_predictions, emotions)\n",
    "\n",
    "test_supports = np.array(test_labels).sum(axis=0).astype(int)\n",
    "test_supports = np.append(test_supports,[test_supports.sum()]*4)\n",
    "\n",
    "val_supports = np.array(val_labels).sum(axis=0).astype(int)\n",
    "val_supports = np.append(val_supports,[val_supports.sum()]*4)\n",
    "\n",
    "val_results[\"Support\"] = val_supports\n",
    "test_results[\"Support\"] = test_supports\n",
    "val_results.columns = val_results.columns.map(lambda x: x+\" \")\n",
    "print(pd.concat([val_results, test_results], axis=1, join=\"inner\").to_latex(float_format=\"%.4f\", column_format=\"l|rrrr|rrrr\"))\n",
    "\n",
    "\n",
    "# default_threshold_results.columns = default_threshold_results.columns.map(lambda x: x+\" \")\n",
    "# joined_results = pd.concat([default_threshold_results, custom_threshold_results], axis=1, join=\"inner\")\n",
    "# print(joined_results.to_latex(float_format=\"%.4f\", column_format=\"l|rrrr|rrrr\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_supports = np.array(test_labels).sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions.sum(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(np.array(test_labels), test_predictions, ([e.capitalize() for e in emotions]),\n",
    "                  supports = test_supports, title = \"Zero-shot LLaVA\", save_name=f\"final_models/llava/conf_matrix_custom.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MMSA",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
